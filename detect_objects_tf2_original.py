# -*- coding: utf-8 -*-
"""detect_objects_tf2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ihlQV5TNXr2AMiZlvH3O7tRhJGONpuF9
"""

# Commented out IPython magic to ensure Python compatibility.
# # Install
# %%bash
# pip install -U tensorflow=="2.*" #upgrade to most recent stable version
# pip install tf_slim
# # Make sure pycocotools is installed
# pip install pycocotools

# Clone the tensorflow models repository
# "--depth 1" copy only the latest revision of a repo
!git clone --depth 1 https://github.com/tensorflow/models

!ls

# Commented out IPython magic to ensure Python compatibility.
# # Compile protobufs
# # sudo apt install -y protobuf-compiler -- didn't need...
# %%bash
# cd models/research/
# protoc object_detection/protos/*.proto --python_out=.

!ls models/research/object_detection/protos/

# Commented out IPython magic to ensure Python compatibility.
# # install the object_detection package
# %%bash 
# cd models/research
# pip install .

# Commented out IPython magic to ensure Python compatibility.
# base imports
import numpy as np
import os
import pathlib
import six.moves.urllib as urllib
import sys
import tarfile
import tensorflow as tf
print(tf.__version__)

import zipfile
import six

from collections import defaultdict
from io import StringIO
from matplotlib import pyplot as plt
from PIL import Image
from IPython.display import display

# %matplotlib inline

# Import the object detection module
from object_detection.utils import ops as utils_ops
from object_detection.utils import label_map_util
from object_detection.utils import visualization_utils as vis_util

"""# Model preparation

## Variables

Any model exported using the `export_inference_graph.py` tool can be loaded here simply by changing the path.
"""

# Download the saved model and put it into models/research/object_detection/test_data/
!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/faster_rcnn_inception_resnet_v2_640x640_coco17_tpu-8.tar.gz
!tar -xf faster_rcnn_inception_resnet_v2_640x640_coco17_tpu-8.tar.gz
!mv faster_rcnn_inception_resnet_v2_640x640_coco17_tpu-8/ models/research/object_detection/test_data/

# Download the saved model and put it into models/research/object_detection/test_data/
!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d5_coco17_tpu-32.tar.gz
!tar -xf efficientdet_d5_coco17_tpu-32.tar.gz
!mv efficientdet_d5_coco17_tpu-32/ models/research/object_detection/test_data/

# Download the saved model and put it into models/research/object_detection/test_data/
!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d1_coco17_tpu-32.tar.gz
!tar -xf efficientdet_d1_coco17_tpu-32.tar.gz
!mv efficientdet_d1_coco17_tpu-32/ models/research/object_detection/test_data/

!ls models/research/object_detection/test_data/

# Loader
def load_model(model_name):
  directory = 'models/research/object_detection/test_data/'
  model_dir = directory + model_name + "/saved_model"
  if os.path.exists(model_dir):
    model = tf.saved_model.load(model_dir)
    return model
  else:
    # exits the program
    sys.exit('model not loaded')

"""## Loading label map
Label maps map indices to category names, so that when our convolution network predicts `5`, we know that this corresponds to `airplane`.  Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine
"""

# List of the strings that is used to add correct label for each box.
PATH_TO_LABELS = 'models/research/object_detection/data/mscoco_label_map.pbtxt'
category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)

!ls models/research/object_detection/test_images

# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.
PATH_TO_TEST_IMAGES_DIR = pathlib.Path('models/research/object_detection/test_images')
TEST_IMAGE_PATHS = sorted(list(PATH_TO_TEST_IMAGES_DIR.glob("*.jpg")))
TEST_IMAGE_PATHS

"""# Detection"""

# Load an object detection model
# chose a model from models/research/object_detection/test_data/
model_name = 'efficientdet_d1_coco17_tpu-32'
detection_model = load_model(model_name)

# Check the model's input signature, it expects a batch of 3-color images of type uint8
print(detection_model.signatures['serving_default'].inputs)

# Check the model's outputs
detection_model.signatures['serving_default'].output_dtypes

# Check the model's output shapes
detection_model.signatures['serving_default'].output_shapes

# Add a wrapper function to call the model, and cleanup the outputs

def run_inference_for_single_image(model, image):
  image = np.asarray(image)
  # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.
  input_tensor = tf.convert_to_tensor(image)
  # The model expects a batch of images, so add an axis with `tf.newaxis`.
  input_tensor = input_tensor[tf.newaxis,...]

  # Run inference
  model_fn = model.signatures['serving_default']
  output_dict = model_fn(input_tensor)

  # All outputs are batches tensors.
  # Convert to numpy arrays, and take index [0] to remove the batch dimension.
  # We're only interested in the first num_detections.
  num_detections = int(output_dict.pop('num_detections'))
  output_dict = {key:value[0, :num_detections].numpy() 
                 for key,value in output_dict.items()}
  output_dict['num_detections'] = num_detections

  # detection_classes should be ints.
  output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)
  #print(output_dict['num_detections']) 
  #print(output_dict['detection_classes']) 
  #print(output_dict['detection_boxes']) 
  #print(output_dict['detection_scores']) 
      
  return output_dict

# Define and retrieve attributes of objects identified in images
def get_detections(image_name, image, boxes, classes, scores, cat_index, min_score_thresh):
    im_width, im_height = image.size
    detections = []
    for i in range(boxes.shape[0]):
        if scores is None or scores[i] > min_score_thresh:
            box = tuple(boxes[i].tolist())
            ymin, xmin, ymax, xmax = box
            (left, right, top, bottom) = (xmin * im_width,
                                          xmax * im_width,
                                          ymin * im_height,
                                          ymax * im_height)
            if classes[i] in cat_index.keys():
                class_name = cat_index[classes[i]]['name']
            else:
                class_name='N/A'
            detections.append(
                {'image_id': image_name,
                 'object': class_name,
                 'coordinates': {
                     'left': left,
                     'right': right,
                     'bottom': bottom,
                     'top': top
                 },
                 'score': scores[i]
                 }
            )
    return detections

# Run inference on each test image and show the results

# Set min_score_thresh to other values (between 0 and 1) to allow more 
# detections in or to filter out more detections.

overall_detections = [] 
def show_inference(model, image_path):
  # the array based representation of the image will be used later in order to prepare the
  # result image with boxes and labels on it.
  image_np = np.array(Image.open(image_path))
  # Actual detection.
  output_dict = run_inference_for_single_image(model, image_np)
  # Visualization of the results of a detection.
  threshold = 0.3 # set minimun score threshold
  vis_util.visualize_boxes_and_labels_on_image_array(
      image_np,
      output_dict['detection_boxes'],
      output_dict['detection_classes'],
      output_dict['detection_scores'],
      category_index,
      use_normalized_coordinates=True,
      max_boxes_to_draw=200,
      min_score_thresh=threshold,
      line_thickness=8)
  
  image_name = os.path.basename(image_path)
  image = Image.open(image_path)

  detections = get_detections(
      image_name,
      image,
      output_dict['detection_boxes'],
      output_dict['detection_classes'],
      output_dict['detection_scores'],
      category_index,
      threshold)
  
  overall_detections.extend(detections)      
  print(detections)

  
  plt.figure(figsize=(24,32))
  plt.imshow(Image.fromarray(image_np))
  plt.show()
  #display(Image.fromarray(image_np))

# Run inference for all images in TEST_IMAGE_PATHS directory and plot 
# average time per image

import time
import pandas as pd

elapsed = []
for image_path in TEST_IMAGE_PATHS:
  start_time = time.time()
  show_inference(detection_model, image_path)
  end_time = time.time()
  elapsed.append(end_time - start_time)

display(pd.DataFrame(overall_detections))
mean_elapsed = sum(elapsed) / float(len(elapsed))
print('Elapsed time: ' + str(mean_elapsed) + ' second per image')

# ======= Count objects of interest =========

# Dataframe with image detections from pretrained model
df = pd.DataFrame(overall_detections)
# group by image and type of object and perform counts
objects_of_interest = ['bicycle', 'car', 'person', 'motorcycle', 'bus', 'truck']
df = df[df.object.isin(objects_of_interest)]
df = df[['image_id', 'object']]
counts = df.groupby(['image_id', 'object']).size().to_frame('counts').reset_index()
counts